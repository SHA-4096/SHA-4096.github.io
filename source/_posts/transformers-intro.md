---
title: ğŸ¤— Transformers
date: 2025/5/7
tags:
    - AI
category: æŠ€æœ¯å­¦ä¹ ä¸åˆ†äº«
---
å¤§å®¶å¥½å•Šï¼Œä»Šå¤©æ¥èŠä¸€èŠtransformers...è¿™é‡ŒæŒ‡çš„ä¸æ˜¯ Attention is All You Need é‡Œæå‡ºçš„transformersç†è®ºï¼Œè€Œæ˜¯ç”±Huggingfaceå¼€æºçš„ä¸€ä¸ªå«ğŸ¤— Transformers çš„åº“

transformersæ˜¯ä¸€ä¸ªå·¨å¤§æ— æ¯”çš„ä»“åº“ï¼Œå¤§é‡ä¸»æµçš„å¼€æºæ¨¡å‹éƒ½åœ¨transformersä¸Šæœ‰ä»£ç å®ç°ã€‚è€Œä¸”ï¼Œåƒvllmè¿™æ ·çš„æ¨ç†æ¡†æ¶ï¼Œä¹Ÿæ˜¯åŸºäºtransformerså¼€å‘çš„ã€‚ç®€å•æ¥è¯´ï¼Œé€šè¿‡transformersï¼Œä½ å¯ä»¥ï¼ˆç›¸å¯¹ï¼‰è½»æ¾åœ°è°ƒç”¨å„ç§å¼€æºæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæˆ–è€…è®­ç»ƒ/å¾®è°ƒå‡ºé€‚åˆè‡ªå·±çš„æ¨¡å‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œtransformerså®Œå…¨å¼€æºï¼Œå¹¶ä¸”æœ‰ç€ç›¸å¯¹æ¸…æ™°çš„é¡¹ç›®ç»“æ„ï¼Œä¸ç®¡æ˜¯ä½œä¸ºå­¦ä¹ å¤§æ¨¡å‹çš„ææ–™è¿˜æ˜¯æ–°çš„æ¨¡å‹å¼€å‘çš„æ¡†æ¶éƒ½éå¸¸åˆé€‚ã€‚

ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†

é¦–å…ˆè¦è¿›è¡Œç¯å¢ƒé…ç½®
pip install transformers datasets evaluate accelerate

pip install torch

ä½¿ç”¨transformersæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯é€šè¿‡pipelineç±»ã€‚åŸºæœ¬ä¸Šï¼Œtransformerså°†å¤§æ¨¡å‹çš„ä»»åŠ¡åˆ†æˆäº†å‡ ç±»ï¼Œä½ åªéœ€è¦åœ¨åˆå§‹åŒ–pipelineçš„æ—¶å€™å‘Šè¯‰å®ƒä½ è¦æ‰§è¡Œä»€ä¹ˆä»»åŠ¡ï¼Œå®ƒå°±ä¼šå¸®ä½ åˆå§‹åŒ–é»˜è®¤çš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç„¶ååªè¦æŠŠè¾“å…¥é€šè¿‡å‚æ•°ä¼ é€’ç»™pipelineå®ä¾‹å°±å¯ä»¥å¾—åˆ°æ¨ç†ç»“æœäº†ã€‚æ¯”å¦‚ä¸‹é¢çš„ä»£ç å°±æ˜¯ç”¨pipelineè°ƒç”¨ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„å®ä¾‹

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

print(classifier("We are very happy to show you the ğŸ¤— Transformers library."))

#[{'label': 'POSITIVE', 'score': 0.9998}]
```

pipelineæ”¯æŒçš„ä»»åŠ¡å¦‚ä¸‹è¡¨ï¼š
æš‚æ—¶æ— æ³•åœ¨é£ä¹¦æ–‡æ¡£å¤–å±•ç¤ºæ­¤å†…å®¹

è¿™ä¸ªæ—¶å€™æˆ‘ä»¬ä¸éš¾å‘ç°ï¼Œpipielineæ˜¯ä¸€ä¸ªç±»ä¼¼interfaceçš„ä¸œè¥¿ï¼ŒæŒ‰ç…§ä¸åŒçš„ä»»åŠ¡æä¾›äº†ä¸åŒçš„â€œæ¥å£â€ã€‚é‚£è‡ªç„¶åœ°ï¼Œæˆ‘ä»¬å¤§éƒ¨åˆ†æ—¶å€™ä¼šæƒ³è¦æŒ‡å®šæŸä¸ªä»»åŠ¡æ‰€ä½¿ç”¨çš„å…·ä½“çš„æ¨¡å‹ï¼Œé€šè¿‡ç»™pipelineä¼ é€’modelå‚æ•°å¯ä»¥å®ç°è¿™ä¸€ç‚¹ã€‚

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="meta-llama/Llama-3.2-3B")

ç”¨æ›´åº•å±‚çš„æ–¹æ³•è¿›è¡Œæ¨ç†

pipelineåªæ˜¯transformersåŸºæœ¬çš„æ¨ç†åŠŸèƒ½çš„ä¸€ä¸ªé«˜å±‚å°è£…ï¼Œå¦‚æœæƒ³è¦æ›´å¥½åœ°åˆ©ç”¨transformersè¿›è¡Œæ¨ç†ï¼Œå°±åº”è¯¥åœ¨ä»£ç ä¸­æ‰‹åŠ¨å®ç°ä»¥ä¸‹æ­¥éª¤ï¼š

- åŠ è½½æ¨¡å‹(Model)
- åŠ è½½åˆ†è¯å™¨(Tokenizer)
- å‡†å¤‡æ•°æ®
- ä½¿ç”¨åˆ†è¯å™¨å¯¹æ•°æ®è¿›è¡Œåˆ†è¯ï¼Œå¾—åˆ°æ¨¡å‹å¯ä»¥ç”¨æ¥è®¡ç®—çš„input
- å°†æ•°æ®ä¼ å…¥æ¨¡å‹ï¼Œå¼€å§‹æ¨ç†
- è·å¾—æ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶decode(tokenizer.decode)

å…·ä½“åœ°ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ç”¨llamaæ¥è¿›è¡Œå¯¹è¯ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š
from transformers import AutoTokenizer, AutoModelForCausalLM

path="meta-llama/Llama-3.1-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path)

text="Hey, how many 'r's are there in the word 'strawberry'?"

messages = [
  {"role": "system", "content": "You are a bot that responds to questions"},
  {"role": "user", "content": text}
]

#è¿™é‡Œåšäº†ä¸¤ä¸ªæ¯”è¾ƒé‡è¦çš„äº‹æƒ…ï¼Œä¸€ä¸ªæ˜¯ä¸ºè¾“å…¥æ·»åŠ special tokensï¼Œä¸€ä¸ªæ˜¯å°†è¾“å…¥å˜ä¸ºè¯å‘é‡
prompt = tokenizer.apply_chat_template(messages, \
    add_generation_prompt=True, tokenize=False) 

input = tokenizer(prompt)

#model.gererateæ˜¯è°ƒç”¨æ¨¡å‹çš„ä¸»è¦æ–¹æ³•
output = model.generate(
    **input,
    max_new_tokens=50, #é™åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡
)

context_length = input.input_ids.shape[-1]
sequence = output['sequences'][0][context_length:]
pred = tokenizer.decode(sequence, skip_special_tokens=True)
print(pred)

å¾®è°ƒ
ä½¿ç”¨ ğŸ¤— Transformers çš„ Trainer å¯ä»¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚ä»¥bertæ¨¡å‹çš„å¾®è°ƒä¸ºä¾‹ï¼š

from datasets import load_dataset

# é¦–å…ˆåŠ è½½æ•°æ®é›†
dataset = load_dataset("yelp_review_full")
dataset["train"][100]

# ç„¶ååŠ è½½åˆ†è¯å™¨ï¼Œç”¨æ¥å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# ç”¨dataset.mapå¯¹æ•°æ®é›†è¿›è¡Œé‡æ–°æ˜ å°„
tokenized_datasets = dataset.map(tokenize_function, batched=True)

#å–ä¸€å°éƒ¨åˆ†çš„å­é›†è¿›è¡Œè®­ç»ƒ
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

# ä½¿ç”¨trainerè¿›è¡Œè®­ç»ƒ
# é¦–å…ˆåŠ è½½æ¨¡å‹
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)

# ç„¶åè®¾ç½®ä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼Œè¿™é‡Œæ²¡æœ‰å¯¹è®­ç»ƒçš„å‚æ•°åšä¿®æ”¹ï¼Œä»…ä»…æ˜¯æŒ‡å®šäº†è¾“å‡ºç›®å½•
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")

# æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥è¯„ä¼°è®­ç»ƒæ•ˆæœï¼Œå¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªå«evaluateçš„åº“æä¾›äº†ä¸€ä¸ªaccuracyå‡½æ•°ï¼Œæˆ‘ä»¬å°±ä¸ç”¨è‡ªå·±æ‰‹å†™å•¦
import numpy as np
import evaluate
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred): #è¿™ä¸ªå‡½æ•°ä¼šè¢«ä¼ é€’åˆ°Traineré‡Œ
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
    

# ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªtrainerå¹¶è°ƒç”¨å®ƒè¿›è¡Œè®­ç»ƒå³å¯
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
ç†è§£Special Tokens
ä½ å¯èƒ½ä¼šæœ‰äº›å¥½å¥‡ï¼Œapply_chat_templateå¯¹æˆ‘ä»¬çš„è¾“å…¥åšäº†ä»€ä¹ˆä¿®æ”¹ã€‚å®é™…ä¸Šï¼Œä¸»è¦çš„ä¿®æ”¹ä½“ç°åœ¨special tokençš„æ·»åŠ ä¸Š

ä»€ä¹ˆæ˜¯special token? åŸºæœ¬ä¸Šï¼Œå®ƒä»¬æ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åˆ°çš„å…·æœ‰ç‰¹æ®Šæ„ä¹‰çš„tokenï¼Œç”¨æ¥æ ‡è¯†å„ç§ä¸å¯¹è¯ç›¸å…³çš„ä¿¡æ¯ã€‚æ¯”å¦‚ï¼Œllama3.1çš„éƒ¨åˆ†special tokenså¦‚ä¸‹ï¼š

![alt text](attachments/special-tokens.png)
è€Œæˆ‘ä»¬ä¸Šé¢çš„å¯¹è¯ï¼Œåœ¨åŠ äº†special tokensä¹‹åçš„æ–‡æœ¬çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a bot that responds to questions.

<|eot_id|><|start_header_id|>user<|end_header_id|>

Hey, how many 'r's are there in the word 'strawberry'?<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
ä¸ºä»€ä¹ˆè¦åŠ ä¸Šspecial tokenå‘¢ï¼Ÿ

æˆ‘ä»¬ä»¥ä¸Šé¢ä¾‹å­ä¸­å‡ºç°çš„special tokensä¸ºä¾‹å­è§£é‡Šã€‚æˆ‘ä»¬é¦–å…ˆè¦æ˜ç¡®ä¸€ç‚¹ï¼Œtransformersæ¶æ„çš„å¤§æ¨¡å‹çš„å·¥ä½œæ–¹å¼ï¼Œæ˜¯æ ¹æ®å·²æœ‰æ–‡æœ¬ï¼Œè¡¥å…¨ä¸‹ä¸€ä¸ªæœ€æœ‰å¯èƒ½å‡ºç°çš„æ–‡æœ¬ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœå‘æ¨¡å‹è¾“å…¥1+1=ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªæœ€æœ‰å¯èƒ½çš„æ–‡æœ¬å°±æ˜¯2ã€‚å¯¹äºé—®ç­”ä»»åŠ¡æ¥è¯´ï¼Œæˆ‘ä»¬å¹¶ä¸å¸Œæœ›æ¨¡å‹â€œè¡¥å…¨â€æˆ‘ä»¬çš„é—®é¢˜ï¼Œè€Œæ˜¯å¸Œæœ›æ¨¡å‹èƒ½å¯¹æˆ‘ä»¬çš„é—®é¢˜åšå‡ºå›ç­”ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å°±è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†å¼€å±äºä¸åŒè§’è‰²çš„æ–‡æœ¬ï¼Œ<|start_header_id|>å’Œ<|end_header_id|>å°±èµ·åˆ°äº†è¿™æ ·çš„ä½œç”¨ã€‚è¿™æ ·è®­ç»ƒä¹‹åï¼Œæ¨¡å‹å°±çŸ¥é“äº†ï¼Œåœ¨<|start_header_id|>assistant<|end_header_id|>è¿™ä¸€è¡Œçš„åé¢ï¼Œéœ€è¦è¡¥å…¨çš„æ˜¯æ¨¡å‹çš„å›ç­”ï¼Œè€Œä¸æ˜¯ç”¨æˆ·æå‡ºçš„é—®é¢˜çš„å»¶ç»­ã€‚

å¯ä»¥å°è¯•å»æ‰ä¸Šé¢ä¾‹å­ä¸­çš„apply_chat_templateé‚£ä¸€è¡Œï¼Œç„¶åçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ

å…³äºtransformersçš„ä»£ç å®ç°
transformersçš„ä»£ç ç»“æ„ï¼Œå¤§è‡´å°±æ˜¯ä»GenerationMixinç±»å¼€å§‹ï¼Œä¸€å±‚ä¸€å±‚åœ°å¾€ä¸‹åµŒå¥—ï¼š
![alt text](attachments/image-2.png)
![alt text](attachments/image-3.png)
å¦‚æœæˆ‘ä»¬æƒ³è¦å¯¹åº•å±‚çš„ä¸€äº›ä»£ç è¿›è¡Œä¿®æ”¹çš„è¯ï¼Œä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨pythonçš„ä¸€ä¸ªå«åšmonkeypatchçš„è¯­è¨€ç‰¹æ€§ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ–¹æ³•ï¼Œä½¿å¾—forwardå‡½æ•°è¢«è°ƒç”¨æ—¶ï¼Œè·³è½¬åˆ°æˆ‘ä»¬è‡ªå®šä¹‰çš„forwardå‡½æ•°ï¼š
import transformers

def my_prepare_inputs_for_generation_llama(
    self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs
):
    ...
    return model_inputs
def original_llama_flash_attn_2_forward(
    self,
    hidden_states: torch.Tensor,
    ...
):
    return attn_output, attn_weights, past_key_value

def replace_llama():
    transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation = my_prepare_inputs_for_generation_llama
    transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward = my_flash_attn_forward_llama
    
replace_llama()

å‚è€ƒ
https://huggingface.co/docs/transformers/main/zh/index
https://huggingface.co/docs/transformers/main/zh/quicktour
https://huggingface.co/docs/transformers/main/zh/training
https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
https://github.com/huggingface/transformers
https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/
https://www.n2ptr.space/2025/03/18/how_does_model_generate_work/
